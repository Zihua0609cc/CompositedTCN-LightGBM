
import pandas as pd


from tensorflow.python.keras.layers import BatchNormalization
import lightgbm as lgb

import CTCN_MODEL_TRAIN.Read_DataSet
from keras import Input, Model
import matplotlib.pyplot as plt
import  os
from keras.optimizers import adam_v2
from keras.layers import Conv1D, LeakyReLU, add, Activation, Flatten, Dense, Dropout

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

from timeit import default_timer as timer



def Read_Excel():
    p_num = 76
    p_tnum = 85
    p_length = 118
    
    data_A = CTCN_MODEL_TRAIN.Read_DataSet.get_data('dataset.xlsx', 'Sheet1')
    data_A = MinMaxScaler().fit_transform(data_A)
    label_A = CTCN_MODEL_TRAIN.Read_DataSet.get_label('dataset.xlsx', 'Sheet1',9)

    dataset_split = p_num * p_length
    # 模型训练集数目和标签 Model training dataset number and labels
    train_data = data_A[:dataset_split - 1]
    train_label = label_A[:dataset_split - 1]

    # 模型测试集中某118个连续数据项和标签   The  118 continuous data items and labels in the model test dataset
    test_data = data_A[dataset_split:p_length * p_tnum]
    test_label = label_A[dataset_split:len(data_A)]

    train_data = train_data.reshape(-1, 1, 9)
    test_data = test_data.reshape(-1, 1, 9)
    return train_data, test_data, train_label, test_label


optimizer = adam_v2.Adam(0.0002, 0.5)
# CTCN 复合协调特征提取  Composited collaborative feature extraction by the NONBN-CTCN
def ResBlock(x,filters,kernel_size,dilation_rate, b):
    r = Conv1D(filters,kernel_size,padding='same',dilation_rate=dilation_rate)(x)
    r = BatchNormalization(momentum=0.9)(r)
    r = LeakyReLU(alpha=0.15)(r)
    r = Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation_rate)(r)
    r = BatchNormalization(momentum=0.9)(r)

    x = LeakyReLU(0.6)(x)
    x = Conv1D(filters/b,kernel_size,padding='same')(x)
    shortcut = Conv1D(filters,kernel_size=1,padding='same')(x)
    #  输出和原始输入相加 Add outputs and original inputs
    o = add([r,shortcut])
    o = Activation('relu')(o)
    return o


def NONB_CTCN_PREDICTION():

    # STCN 特征提取
    inputs = Input(shape=(1, 9))
    x = ResBlock(inputs, filters=128, kernel_size=2, dilation_rate=1, b=4)
    x = ResBlock(x, filters=64, kernel_size=2, dilation_rate=2, b=4)
    x = ResBlock(x, filters=32, kernel_size=2, dilation_rate=4, b=4)
    x = ResBlock(x, filters=16, kernel_size=2, dilation_rate=8, b=4)
    x = ResBlock(x, filters=16, kernel_size=2, dilation_rate=16, b=4)
    c1 = Flatten()(x)
    c2 = Dense(units=32)(c1)
    c3 = LeakyReLU(alpha=0.15)(c2)
    c4 = Dropout(0.15)(c3)
    outputs = Dense(units=16)(c4)
    return Model(inputs, outputs)


    # print(model.summary())
    # model.fit(train_data, train_label, validation_split=0.1, epochs=500, batch_size=50, verbose=2)
    # model.save('D:/Pthird_nn/My_TCN_GBDT_Dataset/' + 'TCN_PREDICTION_Model.h5')
def LIGHT_GBM(train_features, train_labels):
    Light_GBM = lgb.LGBMRegressor(n_estimators= 500, metric='mse',learning_rate=0.1,max_depth=6, bagging_fraction = 0.5,feature_fraction = 0.9)
    Light_GBM.fit(train_features, train_labels)
    return Light_GBM

def show_picture(test_label, Prediction_data):
    plt.figure()
    # 1*1 的网络，第一幅子图
    plt.plot(range(len(test_label)),test_label, linestyle='-',  color='r', linewidth=1,marker='o',markersize=3,  label="Actual_data")
    plt.plot(range(len(Prediction_data)),Prediction_data,linestyle='-',  color='b', linewidth=1,marker='o',markersize=3,  label="Prediction_data")
    plt.show()

def RMSE_Model(Prediction_data, test_label):

    rmse = np.sqrt(mean_squared_error(Prediction_data, test_label))
    return rmse

def MAE_Model(Prediction_data, test_label):
    # error = []
    # for i in range(len(Prediction_data)):
    #     error.append(abs(Prediction_data[i] - test_label[i]))

    # mae = np.average(np.abs(Prediction_data - test_label))
    mae = mean_absolute_error(Prediction_data,test_label)
    return mae

def mape_loss_func(preds,labels):
    mask=labels!=0
    return np.fabs((labels[mask]-preds[mask])/labels[mask]).mean()


def NSE_Model(Prediction_data, test_label):
    a1 = np.sum((test_label - np.mean(test_label))**2)
    a2 = np.sum((Prediction_data - test_label)**2)
    NSE = 1 - a1/a2
    return  NSE



def MAPE_Model(Prediction_data, test_label):
    mask = test_label != 0
    Prediction_data = np.reshape(Prediction_data,(-1,1))
    test_label = np.reshape(test_label,(-1,1))
    MAPE = np.fabs((test_label[mask] - Prediction_data[mask]) / test_label[mask]).mean()
    # MAPE = mean_absolute_percentage_error(Prediction_data,test_label)
    return MAPE


# 保存数据
def SAVEDATA_CSV(test_label, Prediction_data):
    df = pd.DataFrame()
    df['label'] = test_label
    df['Prediction'] = Prediction_data
    df.to_csv("C:\\Users\\ChenZihua\\Desktop\\1.csv",header=None,index=None)



#模型主体预测
train_data, test_data, train_label, test_label= Read_Excel()
TCN_Extracted = NONB_CTCN_PREDICTION()
TCN_Extracted.compile(loss='mse', optimizer=optimizer)
#118个样本训练一次,与118个连续装载数据匹配 118 samples are trained once, match with 118 consecutive loading data
train_history = TCN_Extracted.fit(train_data, train_label, validation_split=0.01, epochs=500, batch_size= 118, verbose=2)

#Model 存储  Model save
# TCN_Extracted.save('D:/Pthird_nn/My_TCN_GBDT_Dataset/' + 'TCN_PREDICTION_NONBN_Model.h5')
# TCN_Extracted = load_model('D:/Pthird_nn/My_TCN_GBDT_Dataset/TCN_PREDICTION_NONBN_Model.h5')

Features_Data_1 = TCN_Extracted.predict(train_data)
start = timer()
Features_Data_2 = TCN_Extracted.predict(test_data)
end1 = timer()
Light_gbm = LIGHT_GBM(Features_Data_1, train_label)
end2 = timer()
Final_Prediction = Light_gbm.predict(Features_Data_2)
end3 = timer()

# RMSE metrics function
RMSE = RMSE_Model(Final_Prediction, test_label)
# MAE metrics function
MAE = MAE_Model(Final_Prediction, test_label)

#r2_score metrics
R2 = r2_score(Final_Prediction, test_label)
print("RMSE", RMSE, "MAE", MAE,"R2", R2)

# 计算运算时间 Calculate the computational time
print(end3-start-(end2-end1))
show_picture(test_label, Final_Prediction)



# def show_train_history(train_history, train, validation):
#     plt.plot(train_history.history[train])
#     plt.plot(train_history.history[validation])
#     plt.title('Train_history')
#     plt.ylabel(train)
#     plt.yscale('log')
#     plt.xlabel('Epoch')
#     plt.legend(['train', 'validation'], loc = 'upper left')
#     plt.show()


# show_train_history(train_history, 'loss', 'val_loss')




